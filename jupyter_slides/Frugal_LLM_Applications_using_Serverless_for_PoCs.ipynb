{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40d3fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Building Frugal <code>OpenSource LLM</code>  Applications <br><br>using <code>Serverless Cloud</code> </h2>\n",
    "<h5 align=\"center\">Useful for PoCs and Batch Processing Jobs</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced4eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Motivation</h2>\n",
    "\n",
    "- Want to build LLM applications? \n",
    "- Wondering what is the most cost effective way to learn and build them in cloud?\n",
    "\n",
    "> Think OpenSource LLM. <br>\n",
    "> Think Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234fe54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Debates that we ARE NOT having today</h2>\n",
    "\n",
    "\n",
    "Or probably could have by the end of session: \n",
    "> OpenSource LLMs vs Paid LLMs <br>\n",
    "> Own Cloud hosted LLM vs Serverless Pay-as-you-go LLM APIs <br>\n",
    "\n",
    "Note: \n",
    "- The above are 2 different debates. \n",
    "- You can pay to use the Serverless AWS Bedrock API and but invoke an Open Source LLM model like `Mistral AI Instruct`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e522d22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Purpose of this Presentation</h2>\n",
    "\n",
    "Let us see how the intermingling of 2 concepts - Serverless + Open Source LLMs - help you build demo-able PoC LLM applications, at minimal cost. \n",
    "\n",
    "\n",
    "```\n",
    "#LLMOps\n",
    "#MLOps\n",
    "#AWSLambda\n",
    "#LLMonServerless\n",
    "#OpenSourceLLMs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc704e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> LLM Recipes we are discussing today: </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482afc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 1) A Lambda to run inference on a purpose-built Transformer ML Model\n",
    "     - A Lambda to **Anonymize Text** using a Huggingface BERT Transformer-based Language Model for PII De-identification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66723ba2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 2) A Lambda to run a **Small Language Model** like Microsoft's Phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78675638",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 3) A Lambda to run a **RAG** Implementation on a Small Language Model like Phi3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43b7c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 4) A Lambda to invoke **a LLM like Mistral 7B Instruct**\n",
    "    -  the LLM is running in  SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c640fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 1. Lambda to Anonymize Text </h2>\n",
    "\n",
    "\n",
    "- A Lambda to run inference on a purpose-built ML Model\n",
    "     - This lambda can **Anonymize Text** \n",
    "     - using a Huggingface BERT Transformer-based Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df308304",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_anonymize_text/container_lambda_with_api_gateway.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11183707",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_anonymize_text/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea620625",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569c05d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 2. Small Language Model </h2>\n",
    "\n",
    "- A Lambda to run a **Small Language Model** like Microsoft's Phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63fdd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_slm/container_lambda_with_api_gateway_diag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f566b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_slm/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa18034",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51ce15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 3. Small Language Model with RAG </h2>\n",
    "\n",
    "- A Lambda to run a RAG Implementation on a Small Language Model like Phi3, that gives better context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ef64c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What is RAG**, **How does RAG improve LLM Accuracy**?\n",
    "\n",
    "> Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. \n",
    "\n",
    "Source: [Databricks](https://www.databricks.com/glossary/retrieval-augmented-generation-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40357ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How does LLM work?**\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/3TBU5BOctjuaPyxuA8PGul/1c1b0b0129be5fef9eaef73063491582/image1.png\" width=\"500\" />\n",
    "\n",
    "Source: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0f600",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How does RAG in LLM work?**\n",
    "\n",
    "<img src=\"https://files.realpython.com/media/Screenshot_2023-10-28_at_2.05.18_PM.92b839a5972b.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "Source: [RealPython Blog: chromadb-vector-database](https://realpython.com/chromadb-vector-database/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ef8ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How is a Vector DB created**\n",
    "\n",
    "<img src=\"how_is_vector_db_created.png\" width=\"700\" />\n",
    "\n",
    "Source: [AnyScale Blog: a-comprehensive-guide-for-building-rag-based-llm-applications](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae96f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Detour: If you wish to use other Vector databases**\n",
    "    \n",
    "<img src=\"https://thedataquarry.com/posts/vector-db-1/vector-db-source-available.png\" width=\"700\" />\n",
    "\n",
    "\n",
    "Source: [Data Quarry Blog: Vector databases - What makes each one different?](https://thedataquarry.com/posts/vector-db-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be34ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_rag_slm/slm_with_rag_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba273317",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- URL we are testing on is from my favorite DL/NLP Researcher. \n",
    "    - https://magazine.sebastianraschka.com/p/understanding-large-language-models\n",
    "    \n",
    "<img src=\"../container_lambda_to_run_rag_slm/article_we_are_using_as_context.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a36bb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_rag_slm/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae7fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e53937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 4. Large Language Model  (A Partial Serverless)</h2>\n",
    "\n",
    "- A Lambda to invoke **a LLM like Mistral 7B Instruct**\n",
    "    -  that is running in  SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe640e06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../lambda_to_invoke_a_sagemaker_endpoint/lambda_to_invoke_sagemaker_endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2064e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../lambda_to_invoke_a_sagemaker_endpoint/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d7886",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b79f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Exploring Some of the Answers from the LLMs</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43a94c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"anonymize_text.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a11cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"phi3_mini_llm_text_cls.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68e184",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"phi_mini_llm_reasoning.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590bf56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"phi3_llm_rag_lora.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629333d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"phi3_llm_rag_recursion_question.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcdc78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Key Challenges Faced</h2>\n",
    "\n",
    "- Serverless could mean we end up with low end cpu architecture. Hence, latency high for RAG LLM implementations\n",
    "- RAG could mean any big context. But converting the RAG context into a vector store will take time. Hence size of the context needs to be lower for \"AWS Lambda\" implementations\n",
    "- Maximum timelimit in Lambda is 30 min. API Gateway times out in 30 seconds. Hence could not be used in RAG LLM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066f53d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> What knowledge you gain by this way of practice?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004636c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**MLOps Concepts**:\n",
    "- Dockerizing ML Applications. What works in your machine works everywhere. More than 70% of the time building these LLM Apps is in perfecting the dockerfile. \n",
    "- The art of storing ML Models in AWS Lambda Containers. Use `cache_dir` well. Otherwise, models get downloaded everytime docker container is created\n",
    "\n",
    "\n",
    "```python\n",
    "os.environ['HF_HOME'] = '/tmp/model' #the only `write-able` dir in AWS lambda = `/tmp`\n",
    "...\n",
    "...\n",
    "your_model=\"ab-ai/pii_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(your_model,cache_dir='/tmp/model')\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(your_model,cache_dir='/tmp/model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fd47c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**AWS Concepts**:\n",
    "- `aws cli` is your friend for shorterning deployments, especially for Serverless\n",
    "- API Gateway is a frustratingly beautiful service. But a combination of `aws cli` and `OpenAPI` spec makes it replicable\n",
    "- AWS Lambda Costing is awesomely cheap for PoCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b9e4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "## AWS Lambda ARM Architecture Costs (assuming you have used up all your free tier)\n",
    "Number of requests: 50 per day * (730 hours in a month / 24 hours in a day) = 1520.83 per month\n",
    "Amount of memory allocated: 10240 MB x 0.0009765625 GB in a MB = 10 GB\n",
    "Amount of ephemeral storage allocated: 5120 MB x 0.0009765625 GB in a MB = 5 GB\n",
    "\n",
    "Pricing calculations\n",
    "1,520.83 requests x 120,000 ms x 0.001 ms to sec conversion factor = 182,499.60 total compute (seconds)\n",
    "10 GB x 182,499.60 seconds = 1,824,996.00 total compute (GB-s)\n",
    "1,824,996.00 GB-s x 0.0000133334 USD = 24.33 USD (monthly compute charges)\n",
    "1,520.83 requests x 0.0000002 USD = 0.00 USD (monthly request charges)\n",
    "5 GB - 0.5 GB (no additional charge) = 4.50 GB billable ephemeral storage per function\n",
    "4.50 GB x 182,499.60 seconds = 821,248.20 total storage (GB-s)\n",
    "821,248.20 GB-s x 0.0000000352 USD = 0.0289 USD (monthly ephemeral storage charges)\n",
    "24.33 USD + 0.0289 USD = 24.36 USD\n",
    "\n",
    "Lambda costs - Without Free Tier (monthly): 24.36 USD\n",
    "```\n",
    "\n",
    "\n",
    "- If I run a `c5.large` (minimal CPU) EC2 instance running throughout the month, cost = 60 USD\n",
    "- If I run a `g4dn.large` (minimal GPU) EC2 instance running throughout the month, cost = 420 USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb65c40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the **LLM Concepts**:\n",
    "- Frameworks: Llama cpp, LangChain, LlamaIndex, Huggingface (and so many more!)\n",
    "- SLMs work well with Reasoning but are too slow/bad for general knowledge questions\n",
    "\n",
    "> Well, it is difficult to keep up with these frameworks. I flick codes. Models are like wines and these frameworks are like bottles. Tthe important thing is the wine more than the bottle. But getting used to how the wines are stored in the bottles help.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ed687",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Next Steps for the author**:\n",
    "\n",
    "- Codes may not fully effecient! We can further reduce cost if run time is reduced\n",
    "\n",
    "<br> For Phi3-Mini-RAG: \n",
    "- Try leveraging a better embedding model (apart from the ancient `Sentence Transformers`)\n",
    "- What about other vector databases - like Pinecone Milvus (we have used opensource Chromodb) here\n",
    "- Rust for LLMs. Rust for Lambda. \n",
    "\n",
    "Sources: \n",
    "- Rust ML Minimalist framework - Candle: https://github.com/huggingface/candle\n",
    "- Rust for LLM - https://github.com/rustformers/llm\n",
    "- Rust for AWS Lambda - https://www.youtube.com/watch?v=He4inXmMZZI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01920b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Next Steps for the reader**:\n",
    "- Replicate the instructions in the given Github links\n",
    "    - Familiarizing Dockerizing of ML Applications\n",
    "    - Provisioning AWS Resources like AWS Lambda, API Gateway using tools like `aws cli` and `OpenAPI`\n",
    "- Explore various other avenues of using LLMs (especially the paid ones). Paid APIs are cake-walk compared to this. But won't give you the depth in implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51339d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://github.com/senthilkumarm1901/serverless_nlp_app\">github.com/senthilkumarm1901/serverless_nlp_app</a></h5>\n",
    "\n",
    "\n",
    "\n",
    "<h2 align=\"center\"> Thank You</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2792eba",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Frugal_LLM_Applications_using_Serverless_for_PoCs.ipynb to slides\n",
      "[NbConvertApp] Writing 611802 bytes to Frugal_LLM_Applications_using_Serverless_for_PoCs.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Frugal_LLM_Applications_using_Serverless_for_PoCs.ipynb --to slides"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
