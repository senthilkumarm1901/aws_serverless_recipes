{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40d3fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Building Frugal <code>OpenSource LLM</code>  Applications <br><br>using <code>Serverless Cloud</code> </h2>\n",
    "<h5 align=\"center\">Useful for PoCs and Batch Processing Jobs</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced4eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Motivation</h2>\n",
    "\n",
    "- Want to build LLM applications? \n",
    "- Wondering what is the most cost effective way to learn and build them in cloud?\n",
    "\n",
    "> Think OpenSource LLM. <br>\n",
    "> Think Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234fe54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Debates that we are reserving for a better day!</h2>\n",
    "\n",
    "\n",
    "Or probably by end of session: \n",
    "> OpenSource LLMs vs Paid LLMs <br>\n",
    "> Own Cloud hosted LLM vs Serverless Pay-as-you-go LLM APIs <br>\n",
    "\n",
    "Note: \n",
    "- These are 2 different debates. \n",
    "- You can pay to the Serverless Bedrock API and use an Open Source LLM model like `Mistral AI Instruct`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e522d22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Purpose of this Presentation</h2>\n",
    "\n",
    "Let us see how the intermingling of 2 concepts - Serverless + Open Source LLMs - help you build demo-able PoC LLM applications, at minimal cost. \n",
    "\n",
    "\n",
    "```\n",
    "#LLMOps\n",
    "#MLOps\n",
    "#AWSLambda\n",
    "#LLMonServerless\n",
    "#OpenSourceLLMs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc704e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> What are we going to build?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482afc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 1) A Lambda to run inference on a purpose-built ML Model\n",
    "     - A Lambda to **Anonymize Text** using a Huggingface BERT Transformer-based Language Model for PII De-identification \n",
    "- 2) A Lambda to run a **Small Language Model** like Microsoft's Phi3\n",
    "- 3) A Lambda to run a **RAG** Implementation on a Small Language Model like Phi3 \n",
    "- 4) A Lambda to invoke **a LLM like Mistral 7B Instruct**\n",
    "    -  the LLM is running in  SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c640fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 1. Lambda to Anonymize Text </h2>\n",
    "\n",
    "\n",
    "- A Lambda to run inference on a purpose-built ML Model\n",
    "     - This lambda can **Anonymize Text** \n",
    "     - using a Huggingface BERT Transformer-based Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df308304",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_anonymize_text/container_lambda_with_api_gateway.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11183707",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_anonymize_text/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea620625",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_anonymize_text/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569c05d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 2. Small Language Model </h2>\n",
    "\n",
    "- A Lambda to run a **Small Language Model** like Microsoft's Phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63fdd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_slm/container_lambda_with_api_gateway_diag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f566b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_slm/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa18034",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_slm/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51ce15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 3. Small Language Model with RAG </h2>\n",
    "\n",
    "- A Lambda to run a RAG Implementation on a Small Language Model like Phi3, that gives better context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be34ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_rag_slm/slm_with_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba273317",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- URL we are testing on is from my favorite DL/NLP Researcher. \n",
    "    - https://magazine.sebastianraschka.com/p/understanding-large-language-models\n",
    "    \n",
    "    \n",
    "![](../container_lambda_to_run_rag_slm/article_we_are_using_as_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a36bb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../container_lambda_to_run_rag_slm/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae7fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/container_lambda_to_run_rag_slm/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e53937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> 4. Large Language Model  (A Partial Serverless)</h2>\n",
    "\n",
    "- A Lambda to invoke **a LLM like Mistral 7B Instruct**\n",
    "    -  that is running in  SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe640e06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../lambda_to_invoke_a_sagemaker_endpoint/lambda_to_invoke_sagemaker_endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2064e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../lambda_to_invoke_a_sagemaker_endpoint/output_in_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d7886",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"50\" />\n",
    "\n",
    "<h5 align=\"center\"><a href=\"https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/\">https://senthilkumarm1901.github.io/aws_serverless_recipes/lambda_to_invoke_a_sagemaker_endpoint/</a></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfe78b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Key Challenges Faced</h2>\n",
    "\n",
    "- Serverless could mean we end up with low end cpu architecture. Hence, latency high for RAG LLM implementations\n",
    "- RAG could mean any big context. But converting the RAG context into a vector store will take time. Hence size of the context needs to be lower for \"AWS Lambda\" implementations\n",
    "- API Gateway times out in 30 seconds. Hence could not be used in RAG LLM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066f53d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> What knowledge you gain by this way of practice?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004636c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**MLOps Concepts**:\n",
    "- Dockerizing ML Applications. What works in your machine works everywhere. More than 70% of the time building these LLM Apps is in perfecting the dockerfile. \n",
    "- The art of storing ML Models in AWS Lambda Containers. Use `cache_dir` well. Otherwise, models get downloaded everytime docker container is created\n",
    "\n",
    "\n",
    "```python\n",
    "os.environ['HF_HOME'] = '/tmp/model' #the only `write-able` dir in AWS lambda = `/tmp`\n",
    "...\n",
    "...\n",
    "your_model=\"ab-ai/pii_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(your_model,cache_dir='/tmp/model')\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(your_model,cache_dir='/tmp/model')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fd47c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**AWS Concepts**:\n",
    "- `aws cli` is your friend for shorterning deployments, especially for Serverless\n",
    "- API Gateway is a frustratingly beautiful service. But a combination of `aws cli` and `OpenAPI` spec makes it replicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb65c40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the **LLM Concepts**:\n",
    "- Frameworks: Llama cpp, LangChain, Huggingface (there are so many I have not used)\n",
    "- SLMs work well with Reasoning but are too slow/bad for general knowledge questions\n",
    "\n",
    "> Well, it is difficult to keep up with these frameworks. I flick codes. Models are like wines and these frameworks are like bottles. Getting used to how the wines are stored in bottles help.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01920b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Next Steps for the reader**:\n",
    "- Replicate the instructions in the given Github links\n",
    "    - Familiarizing Dockerizing of ML Applications\n",
    "    - Provisioning AWS Resources like AWS Lambda, API Gateway using tools like `aws cli` and `OpenAPI`\n",
    "- Explore various other avenues of using LLMs (especially the paid ones). Paid APIs are cake-walk compared to this. But won't give you the depth in implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51339d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Thank You</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2792eba",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Frugal_LLM_Applications_using_Serverless_for_PoCs.ipynb to slides\n",
      "[NbConvertApp] Writing 597633 bytes to Frugal_LLM_Applications_using_Serverless_for_PoCs.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert Frugal_LLM_Applications_using_Serverless_for_PoCs.ipynb --to slides"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
